{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-03T05:23:23.720100Z",
     "start_time": "2024-11-03T05:23:20.138922Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import RobertaTokenizerFast, RobertaForQuestionAnswering\n",
    "\n",
    "tokenizer = RobertaTokenizerFast.from_pretrained('nur-dev/roberta-kaz-large')\n",
    "model = RobertaForQuestionAnswering.from_pretrained('nur-dev/roberta-kaz-large')"
   ],
   "id": "7ef34fa7e3e5ee16",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForQuestionAnswering were not initialized from the model checkpoint at nur-dev/roberta-kaz-large and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-03T05:23:28.312342Z",
     "start_time": "2024-11-03T05:23:23.739182Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from datasets import load_dataset, concatenate_datasets\n",
    "\n",
    "# valid_ds = load_dataset(\"issai/kazqad\", \"kazqad\", split=\"validation\")\n",
    "test_ds = load_dataset(\"issai/kazqad\", \"kazqad\", split=\"test\")\n",
    "test_ds = test_ds.select(range(1000))\n",
    "# dataset = load_dataset(\"Kyrmasch/sKQuAD\", \"kazqad\", split=\"train\")\n",
    "# dataset2 = load_dataset(\"issai/kazqad\", \"nq-translate-kk\", split=\"train\")\n",
    "# \n",
    "# dataset = concatenate_datasets([dataset1, dataset2, valid_ds])"
   ],
   "id": "4bb7ae3743a2259a",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-03T05:23:31.664800Z",
     "start_time": "2024-11-03T05:23:28.388933Z"
    }
   },
   "cell_type": "code",
   "source": "dataset = load_dataset(\"Kyrmasch/sKQuAD\", \"default\", split=\"train\")",
   "id": "a047f3a581a52f76",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-03T05:23:31.674003Z",
     "start_time": "2024-11-03T05:23:31.672174Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def tokenize_function(examples):\n",
    "    return tokenizer(\n",
    "        examples[\"question\"],\n",
    "        examples[\"context\"],\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=384,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_offsets_mapping=True\n",
    "    )"
   ],
   "id": "8a3d40efab72ef11",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-03T05:23:31.729017Z",
     "start_time": "2024-11-03T05:23:31.724520Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def prepare_train_features(examples):\n",
    "    tokenized_examples = tokenizer(\n",
    "        examples[\"question\"],\n",
    "        examples[\"context\"],\n",
    "        truncation=\"only_second\",\n",
    "        max_length=384,\n",
    "        stride=128,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_offsets_mapping=True,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "    \n",
    "    sample_mapping = tokenized_examples.pop(\"overflow_to_sample_mapping\")\n",
    "    offset_mapping = tokenized_examples.pop(\"offset_mapping\")\n",
    "    \n",
    "    # Let's label those examples!\n",
    "    tokenized_examples[\"start_positions\"] = []\n",
    "    tokenized_examples[\"end_positions\"] = []\n",
    "    \n",
    "    for i, offsets in enumerate(offset_mapping):\n",
    "        # We will label impossible answers with the index of the CLS token.\n",
    "        input_ids = tokenized_examples[\"input_ids\"][i]\n",
    "        cls_index = input_ids.index(tokenizer.cls_token_id)\n",
    "        \n",
    "        # Grab the sequence corresponding to that example (to know what is the context and what is the question).\n",
    "        sequence_ids = tokenized_examples.sequence_ids(i)\n",
    "        \n",
    "        # One example can give several spans, this is the index of the example containing this span of text.\n",
    "        sample_index = sample_mapping[i]\n",
    "        answers = examples[\"answer\"][sample_index]\n",
    "        # If no answers are given, set the cls_index as answer.\n",
    "        if len(answers[\"answer_start\"]) == 0:\n",
    "            tokenized_examples[\"start_positions\"].append(cls_index)\n",
    "            tokenized_examples[\"end_positions\"].append(cls_index)\n",
    "        else:\n",
    "            # Start/end character index of the answer in the text.\n",
    "            start_char = answers[\"answer_start\"][0]\n",
    "            end_char = start_char + len(answers[\"text\"][0])\n",
    "            \n",
    "            # Start token index of the current span in the text.\n",
    "            token_start_index = 0\n",
    "            while sequence_ids[token_start_index] != 1:\n",
    "                token_start_index += 1\n",
    "            \n",
    "            token_end_index = len(input_ids) - 1\n",
    "            while sequence_ids[token_end_index] != 1:\n",
    "                token_end_index -= 1\n",
    "            \n",
    "            # Detect if the answer is out of the span (in which case this feature is labeled with the CLS index).\n",
    "            if not (offsets[token_start_index][0] <= start_char and offsets[token_end_index][1] >= end_char):\n",
    "                tokenized_examples[\"start_positions\"].append(cls_index)\n",
    "                tokenized_examples[\"end_positions\"].append(cls_index)\n",
    "            else:\n",
    "                # Otherwise move the token_start_index and token_end_index to the two ends of the answer.\n",
    "                # Note: we could go after the last offset if the answer is the last word (edge case).\n",
    "                while token_start_index < len(offsets) and offsets[token_start_index][0] <= start_char:\n",
    "                    token_start_index += 1\n",
    "                tokenized_examples[\"start_positions\"].append(token_start_index - 1)\n",
    "                \n",
    "                while offsets[token_end_index][1] >= end_char:\n",
    "                    token_end_index -= 1\n",
    "                tokenized_examples[\"end_positions\"].append(token_end_index + 1)\n",
    "    \n",
    "    return tokenized_examples\n"
   ],
   "id": "1b7029cbf5eb7f38",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-03T05:23:31.770885Z",
     "start_time": "2024-11-03T05:23:31.768224Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def add_answer_start(example):\n",
    "    context = example['context']\n",
    "    answer_text = example['answer']\n",
    "    \n",
    "    # Find the position of the answer in the context\n",
    "    start_idx = context.find(answer_text)\n",
    "    \n",
    "    if start_idx == -1:\n",
    "        # Answer not found in context\n",
    "        start_idx = None\n",
    "    example['answers'] = {\n",
    "        'text': [answer_text],\n",
    "        'answer_start': [start_idx] if start_idx is not None else []\n",
    "    }\n",
    "    return example"
   ],
   "id": "add03a2761c6c4b9",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-03T05:23:31.819389Z",
     "start_time": "2024-11-03T05:23:31.815365Z"
    }
   },
   "cell_type": "code",
   "source": "dataset = dataset.map(add_answer_start)",
   "id": "11d292717a601cbc",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-03T05:23:31.876737Z",
     "start_time": "2024-11-03T05:23:31.871637Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def filter_missing_answers(example):\n",
    "    return len(example['answers']['answer_start']) > 0\n",
    "\n",
    "dataset = dataset.filter(filter_missing_answers)"
   ],
   "id": "91a11ab113cfce16",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-03T05:23:31.924215Z",
     "start_time": "2024-11-03T05:23:31.920289Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def prepare_train_features(examples):\n",
    "    tokenized_examples = tokenizer(\n",
    "        examples[\"question\"],\n",
    "        examples[\"context\"],\n",
    "        truncation=\"only_second\",\n",
    "        max_length=384,\n",
    "        stride=128,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_offsets_mapping=True,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "    \n",
    "    sample_mapping = tokenized_examples.pop(\"overflow_to_sample_mapping\")\n",
    "    offset_mapping = tokenized_examples.pop(\"offset_mapping\")\n",
    "    \n",
    "    # Initialize start and end positions\n",
    "    tokenized_examples[\"start_positions\"] = []\n",
    "    tokenized_examples[\"end_positions\"] = []\n",
    "    \n",
    "    for i, offsets in enumerate(offset_mapping):\n",
    "        input_ids = tokenized_examples[\"input_ids\"][i]\n",
    "        cls_index = input_ids.index(tokenizer.cls_token_id)\n",
    "        \n",
    "        sequence_ids = tokenized_examples.sequence_ids(i)\n",
    "        sample_index = sample_mapping[i]\n",
    "        answers = examples[\"answers\"][sample_index]\n",
    "        \n",
    "        if len(answers[\"answer_start\"]) == 0:\n",
    "            # If no answer is found, set start and end positions to CLS index\n",
    "            tokenized_examples[\"start_positions\"].append(cls_index)\n",
    "            tokenized_examples[\"end_positions\"].append(cls_index)\n",
    "        else:\n",
    "            start_char = answers[\"answer_start\"][0]\n",
    "            end_char = start_char + len(answers[\"text\"][0])\n",
    "            \n",
    "            # Find the start and end token indices\n",
    "            token_start_index = 0\n",
    "            while sequence_ids[token_start_index] != 1:\n",
    "                token_start_index += 1\n",
    "            \n",
    "            token_end_index = len(input_ids) - 1\n",
    "            while sequence_ids[token_end_index] != 1:\n",
    "                token_end_index -= 1\n",
    "            \n",
    "            # If the answer is not fully inside the context, label as CLS index\n",
    "            if not (offsets[token_start_index][0] <= start_char and offsets[token_end_index][1] >= end_char):\n",
    "                tokenized_examples[\"start_positions\"].append(cls_index)\n",
    "                tokenized_examples[\"end_positions\"].append(cls_index)\n",
    "            else:\n",
    "                # Move the token indices to the answer boundaries\n",
    "                while token_start_index < len(offsets) and offsets[token_start_index][0] <= start_char:\n",
    "                    token_start_index += 1\n",
    "                start_position = token_start_index - 1\n",
    "                \n",
    "                while offsets[token_end_index][1] >= end_char:\n",
    "                    token_end_index -= 1\n",
    "                end_position = token_end_index + 1\n",
    "                \n",
    "                tokenized_examples[\"start_positions\"].append(start_position)\n",
    "                tokenized_examples[\"end_positions\"].append(end_position)\n",
    "    \n",
    "    return tokenized_examples\n"
   ],
   "id": "b837c94dc6ecde2f",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-03T05:23:32.232478Z",
     "start_time": "2024-11-03T05:23:32.210796Z"
    }
   },
   "cell_type": "code",
   "source": [
    "tokenized_datasets = dataset.map(\n",
    "    prepare_train_features,\n",
    "    batched=True,\n",
    "    remove_columns=dataset.column_names\n",
    ")"
   ],
   "id": "7ee12f5e2ef0258b",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-03T05:23:32.613063Z",
     "start_time": "2024-11-03T05:23:32.592286Z"
    }
   },
   "cell_type": "code",
   "source": [
    "tokenized_datasets = dataset.map(\n",
    "    prepare_train_features,\n",
    "    batched=True,\n",
    "    remove_columns=dataset.column_names\n",
    ")"
   ],
   "id": "2fb7d317312c81fd",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-03T05:23:32.990820Z",
     "start_time": "2024-11-03T05:23:32.963806Z"
    }
   },
   "cell_type": "code",
   "source": [
    "tokenized_test_datasets = test_ds.map(\n",
    "    prepare_train_features,\n",
    "    batched=True,\n",
    "    remove_columns=test_ds.column_names\n",
    ")"
   ],
   "id": "89dd912ffc669a4e",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-03T05:23:33.681733Z",
     "start_time": "2024-11-03T05:23:33.680028Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "7413eb5b80ce182a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-03T05:23:34.498506Z",
     "start_time": "2024-11-03T05:23:34.307082Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    start_logits, end_logits = logits\n",
    "    \n",
    "    start_preds = np.argmax(start_logits, axis=-1)\n",
    "    end_preds = np.argmax(end_logits, axis=-1)\n",
    "    \n",
    "    f1 = f1_score(labels[0], start_preds, average=\"weighted\")\n",
    "    accuracy = accuracy_score(labels[0], start_preds)\n",
    "    \n",
    "    return {\"f1\": f1, \"accuracy\": accuracy}\n"
   ],
   "id": "3edabf2927d89771",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-03T05:23:46.249450Z",
     "start_time": "2024-11-03T05:23:41.879124Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    #save_strategy=\"epoch\",\n",
    "    learning_rate=3e-06,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    adam_beta1=0.8,\n",
    "    adam_beta2=0.999,\n",
    "    num_train_epochs=30,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=50,\n",
    "    report_to=\"tensorboard\",\n",
    "    #deepspeed=\"ds_config.json\",\n",
    "    bf16=True\n",
    ")\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets,\n",
    "    eval_dataset=tokenized_test_datasets,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ],
   "id": "de7986039d54f9a5",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/anaconda3/lib/python3.12/site-packages/transformers/training_args.py:1559: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "/tmp/ipykernel_3879/528224046.py:20: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-11-03 10:23:45,431] [INFO] [real_accelerator.py:219:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/anaconda3/compiler_compat/ld: cannot find -laio: No such file or directory\n",
      "collect2: error: ld returned 1 exit status\n",
      "/home/user/anaconda3/compiler_compat/ld: cannot find -lcufile: No such file or directory\n",
      "collect2: error: ld returned 1 exit status\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-03T05:41:35.473489Z",
     "start_time": "2024-11-03T05:23:46.257884Z"
    }
   },
   "cell_type": "code",
   "source": "trainer.train()",
   "id": "b6a35d9c10169cb3",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4860' max='4860' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4860/4860 17:48, Epoch 30/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.723200</td>\n",
       "      <td>4.299444</td>\n",
       "      <td>0.086783</td>\n",
       "      <td>0.097804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.988900</td>\n",
       "      <td>4.228851</td>\n",
       "      <td>0.114204</td>\n",
       "      <td>0.134731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.378000</td>\n",
       "      <td>4.492192</td>\n",
       "      <td>0.109722</td>\n",
       "      <td>0.119760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.176800</td>\n",
       "      <td>4.677872</td>\n",
       "      <td>0.113187</td>\n",
       "      <td>0.115768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.767300</td>\n",
       "      <td>4.920910</td>\n",
       "      <td>0.112967</td>\n",
       "      <td>0.117764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.565700</td>\n",
       "      <td>5.239927</td>\n",
       "      <td>0.102250</td>\n",
       "      <td>0.106786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.493600</td>\n",
       "      <td>5.703248</td>\n",
       "      <td>0.106928</td>\n",
       "      <td>0.111776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.371100</td>\n",
       "      <td>6.121781</td>\n",
       "      <td>0.112708</td>\n",
       "      <td>0.118762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.218800</td>\n",
       "      <td>6.783863</td>\n",
       "      <td>0.106145</td>\n",
       "      <td>0.100798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.159600</td>\n",
       "      <td>7.432183</td>\n",
       "      <td>0.107202</td>\n",
       "      <td>0.104790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.191400</td>\n",
       "      <td>7.610572</td>\n",
       "      <td>0.105921</td>\n",
       "      <td>0.102794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.153600</td>\n",
       "      <td>8.178177</td>\n",
       "      <td>0.097780</td>\n",
       "      <td>0.093812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.084000</td>\n",
       "      <td>8.228795</td>\n",
       "      <td>0.111338</td>\n",
       "      <td>0.110778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.108900</td>\n",
       "      <td>8.938910</td>\n",
       "      <td>0.106191</td>\n",
       "      <td>0.104790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.166500</td>\n",
       "      <td>8.932434</td>\n",
       "      <td>0.110566</td>\n",
       "      <td>0.112774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.086400</td>\n",
       "      <td>8.764073</td>\n",
       "      <td>0.099847</td>\n",
       "      <td>0.098802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.129900</td>\n",
       "      <td>9.166533</td>\n",
       "      <td>0.111209</td>\n",
       "      <td>0.111776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.101900</td>\n",
       "      <td>9.651373</td>\n",
       "      <td>0.108904</td>\n",
       "      <td>0.110778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.082400</td>\n",
       "      <td>9.861517</td>\n",
       "      <td>0.104012</td>\n",
       "      <td>0.103792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.055900</td>\n",
       "      <td>9.586532</td>\n",
       "      <td>0.101976</td>\n",
       "      <td>0.101796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.106600</td>\n",
       "      <td>9.695106</td>\n",
       "      <td>0.114723</td>\n",
       "      <td>0.115768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.039200</td>\n",
       "      <td>9.711793</td>\n",
       "      <td>0.106898</td>\n",
       "      <td>0.105788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.112500</td>\n",
       "      <td>9.684621</td>\n",
       "      <td>0.107445</td>\n",
       "      <td>0.105788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.056700</td>\n",
       "      <td>9.903811</td>\n",
       "      <td>0.100397</td>\n",
       "      <td>0.099800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.045300</td>\n",
       "      <td>9.979539</td>\n",
       "      <td>0.100091</td>\n",
       "      <td>0.099800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.035600</td>\n",
       "      <td>10.087355</td>\n",
       "      <td>0.101737</td>\n",
       "      <td>0.101796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.061500</td>\n",
       "      <td>9.911821</td>\n",
       "      <td>0.098689</td>\n",
       "      <td>0.098802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.010400</td>\n",
       "      <td>10.028788</td>\n",
       "      <td>0.106494</td>\n",
       "      <td>0.106786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.022100</td>\n",
       "      <td>10.078463</td>\n",
       "      <td>0.107977</td>\n",
       "      <td>0.107784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.024800</td>\n",
       "      <td>10.121859</td>\n",
       "      <td>0.107900</td>\n",
       "      <td>0.107784</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=4860, training_loss=0.406978646350005, metrics={'train_runtime': 1069.1003, 'train_samples_per_second': 18.127, 'train_steps_per_second': 4.546, 'total_flos': 1.349875613058048e+16, 'train_loss': 0.406978646350005, 'epoch': 30.0})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import torch.nn.utils.prune as prune\n",
    "def prune_model(model, amount=0.2):\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, torch.nn.Linear):\n",
    "            # Apply pruning to linear layers\n",
    "            prune.l1_unstructured(module, name='weight', amount=amount)\n",
    "    return model\n"
   ],
   "id": "e7861b2a2ecff80"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "model = prune_model(model, amount=0.2)\n",
    "def remove_pruning(model):\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, torch.nn.Linear):\n",
    "            prune.remove(module, 'weight')\n",
    "    return model\n"
   ],
   "id": "699f1914247b94b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import torch.quantization\n",
    "\n",
    "def quantize_model_dynamic(model):\n",
    "    quantized_model = torch.quantization.quantize_dynamic(\n",
    "        model,  # the model to quantize\n",
    "        {torch.nn.Linear},  # layers to quantize\n",
    "        dtype=torch.qint8  # data type for quantized weights\n",
    "    )\n",
    "    return quantized_model\n"
   ],
   "id": "bc02e971bdcab8d8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "quantized_model = quantize_model_dynamic(model)\n",
    "torch.save(quantized_model.state_dict(), 'quantized_model.pth')\n",
    "model.eval()\n",
    "model.qconfig = torch.quantization.get_default_qconfig('fbgemm')\n",
    "torch.quantization.prepare(model, inplace=True)\n"
   ],
   "id": "43115f42a6a0bb24"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "6c613704269611cc"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
